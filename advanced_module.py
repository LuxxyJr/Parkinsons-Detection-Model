"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ADVANCED ML MODULE â€” Parkinson's Detection Lab                            â•‘
â•‘   XGBoost + LightGBM + CatBoost + Optuna Tuning                            â•‘
â•‘   Stacking Ensemble + SMOTE + Calibration + Uncertainty                     â•‘
â•‘   McNemar + DeLong + Bootstrap CI + Cross-Dataset Validation                â•‘
â•‘                                                                              â•‘
â•‘   Usage: from advanced_module import render_advanced_tabs                   â•‘
â•‘          render_advanced_tabs(df, scaler, FEAT_NAMES, results)              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import LinearSegmentedColormap
import seaborn as sns
from scipy import stats
import warnings, time
warnings.filterwarnings("ignore")

from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_val_score, cross_val_predict
)
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.metrics import (
    accuracy_score, f1_score, roc_curve, auc,
    confusion_matrix, classification_report,
    precision_score, recall_score, matthews_corrcoef,
    average_precision_score, precision_recall_curve,
    brier_score_loss, log_loss
)
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# â”€â”€ Optional heavy imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import xgboost as xgb
    XGB_OK = True
except ImportError:
    XGB_OK = False

try:
    import lightgbm as lgb
    LGB_OK = True
except ImportError:
    LGB_OK = False

try:
    import catboost as cb
    CAT_OK = True
except ImportError:
    CAT_OK = False

try:
    import optuna
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    OPTUNA_OK = True
except ImportError:
    OPTUNA_OK = False

try:
    from imblearn.over_sampling import SMOTE, ADASYN
    from imblearn.combine import SMOTETomek
    SMOTE_OK = True
except ImportError:
    SMOTE_OK = False


# â”€â”€ Colour palette â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WHITE        = "#ffffff"
BG           = "#f4f6f9"
NAVY         = "#1e3a5f"
NAVY_LIGHT   = "#2d5282"
BLUE         = "#2563eb"
BLUE_LIGHT   = "#dbeafe"
GREEN        = "#059669"
GREEN_LIGHT  = "#d1fae5"
RED          = "#dc2626"
RED_LIGHT    = "#fee2e2"
AMBER        = "#d97706"
AMBER_LIGHT  = "#fef3c7"
PURPLE       = "#7c3aed"
PURPLE_LIGHT = "#ede9fe"
SLATE        = "#0ea5e9"
TEAL         = "#0d9488"
BORDER       = "#e2e8f0"
BORDER_MED   = "#cbd5e1"
TEXT_MAIN    = "#1e293b"
TEXT_MID     = "#475569"
TEXT_DIM     = "#94a3b8"

PAL = [BLUE, GREEN, AMBER, PURPLE, RED, SLATE, TEAL, "#f43f5e", "#84cc16", "#fb923c"]

plt.rcParams.update({
    "figure.facecolor": WHITE, "axes.facecolor": WHITE,
    "axes.edgecolor": BORDER_MED, "axes.labelcolor": TEXT_MID,
    "axes.titlecolor": NAVY, "text.color": TEXT_MAIN,
    "xtick.color": TEXT_DIM, "ytick.color": TEXT_DIM,
    "grid.color": BORDER, "grid.linestyle": "--", "grid.alpha": 0.8,
    "font.family": "DejaVu Sans", "axes.titlesize": 11,
    "axes.titleweight": "bold", "axes.labelsize": 9,
    "xtick.labelsize": 8, "ytick.labelsize": 8,
    "legend.fontsize": 8, "legend.facecolor": WHITE,
    "legend.edgecolor": BORDER, "axes.spines.top": False,
    "axes.spines.right": False,
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HELPERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def section(title):
    st.markdown(f"""<div style='font-size:0.7rem;font-weight:700;letter-spacing:2px;
        text-transform:uppercase;color:{TEXT_DIM};border-bottom:2px solid {BORDER};
        padding-bottom:8px;margin-bottom:18px'>{title}</div>""", unsafe_allow_html=True)

def metric_card(val, label, color):
    st.markdown(f"""<div style='background:{WHITE};border:1px solid {BORDER};border-radius:10px;
        padding:18px;text-align:center;border-left:4px solid {color}'>
        <div style='font-family:DM Serif Display,serif;font-size:1.8rem;color:{color};line-height:1.1'>{val}</div>
        <div style='font-size:0.65rem;font-weight:700;letter-spacing:1.5px;
            text-transform:uppercase;color:{TEXT_DIM};margin-top:4px'>{label}</div>
    </div>""", unsafe_allow_html=True)

def info_box(html):
    st.markdown(f"""<div style='background:{BLUE_LIGHT};border-left:4px solid {BLUE};
        padding:14px 18px;border-radius:6px;font-size:0.84rem;
        line-height:1.75;color:{NAVY};margin-bottom:16px'>{html}</div>""",
        unsafe_allow_html=True)

def warn_box(html):
    st.markdown(f"""<div style='background:{AMBER_LIGHT};border-left:4px solid {AMBER};
        padding:10px 16px;border-radius:6px;font-size:0.78rem;
        color:#92400e;font-weight:500;margin-bottom:12px'>{html}</div>""",
        unsafe_allow_html=True)

def success_box(html):
    st.markdown(f"""<div style='background:{GREEN_LIGHT};border-left:4px solid {GREEN};
        padding:10px 16px;border-radius:6px;font-size:0.78rem;
        color:#065f46;font-weight:500;margin-bottom:12px'>{html}</div>""",
        unsafe_allow_html=True)

def get_full_metrics(clf, Xte, yte, name=""):
    yp  = clf.predict(Xte)
    ypr = clf.predict_proba(Xte)[:, 1]
    fpr, tpr, _ = roc_curve(yte, ypr)
    pr_p, pr_r, _ = precision_recall_curve(yte, ypr)
    return dict(
        name=name, clf=clf,
        accuracy =accuracy_score(yte, yp),
        f1       =f1_score(yte, yp),
        precision=precision_score(yte, yp),
        recall   =recall_score(yte, yp),
        roc_auc  =auc(fpr, tpr),
        pr_auc   =average_precision_score(yte, ypr),
        mcc      =matthews_corrcoef(yte, yp),
        brier    =brier_score_loss(yte, ypr),
        logloss  =log_loss(yte, ypr),
        cm       =confusion_matrix(yte, yp),
        report   =classification_report(yte, yp, target_names=["Healthy","PD"]),
        fpr=fpr, tpr=tpr, pr_p=pr_p, pr_r=pr_r,
        y_pred=yp, y_proba=ypr, y_test=yte,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SYNTHETIC UCI TELEMONITORING DATA
#  (Real dataset: archive.ics.uci.edu/dataset/189/parkinsons+telemonitoring)
#  Tsanas et al., IEEE Trans Biomed Eng, 2010
#  5,875 recordings, 42 features, UPDRS motor + total scores
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data
def load_telemonitoring(feat_names, seed=42):
    """
    Synthetic data faithful to UCI Telemonitoring dataset distributions.
    Replace with real parkinsons_updrs.data for production.
    Features overlap with voice dataset (Jitter, Shimmer, NHR, HNR, RPDE, DFA, PPE).
    """
    rng = np.random.RandomState(seed)
    n = 5875
    # ~75% PD (same ratio as voice dataset)
    status = rng.choice([0, 1], size=n, p=[0.25, 0.75])

    specs = {
        "MDVP:Fo(Hz)":      (145.21,35.0,  197.10,45.0),
        "MDVP:Fhi(Hz)":     (197.11,60.0,  243.60,70.0),
        "MDVP:Flo(Hz)":     (102.15,28.0,  146.67,38.0),
        "MDVP:Jitter(%)":   (0.0063,0.004, 0.0033,0.0015),
        "MDVP:Jitter(Abs)": (4.5e-5,2.5e-5,2.2e-5,1e-5),
        "MDVP:RAP":         (0.0033,0.002, 0.0017,0.001),
        "MDVP:PPQ":         (0.0034,0.002, 0.0018,0.001),
        "Jitter:DDP":       (0.0100,0.006, 0.0052,0.003),
        "MDVP:Shimmer":     (0.0508,0.025, 0.0231,0.012),
        "MDVP:Shimmer(dB)": (0.471, 0.22,  0.214, 0.10),
        "Shimmer:APQ3":     (0.0269,0.014, 0.0122,0.006),
        "Shimmer:APQ5":     (0.0316,0.017, 0.0145,0.007),
        "MDVP:APQ":         (0.0439,0.022, 0.0199,0.010),
        "Shimmer:DDA":      (0.0808,0.040, 0.0366,0.018),
        "NHR":              (0.0312,0.028, 0.0111,0.008),
        "HNR":              (19.98, 5.5,   24.68, 4.5),
        "RPDE":             (0.587, 0.085, 0.499, 0.070),
        "DFA":              (0.753, 0.048, 0.718, 0.045),
        "spread1":          (-5.335,1.0,   -6.759,0.9),
        "spread2":          (0.269, 0.10,  0.168, 0.08),
        "D2":               (2.522, 0.38,  2.302, 0.34),
        "PPE":              (0.213, 0.11,  0.062, 0.035),
    }

    rows = {}
    from advanced_module import FEATURE_META_REF  # fallback
    for f in feat_names:
        if f in specs:
            mu_p, sd_p, mu_h, sd_h = specs[f]
            vals = np.where(
                status == 1,
                np.clip(rng.normal(mu_p, sd_p, n), 0, None),
                np.clip(rng.normal(mu_h, sd_h, n), 0, None)
            )
        else:
            vals = rng.normal(0.5, 0.15, n)
        rows[f] = vals

    df = pd.DataFrame(rows)
    df["status"] = status
    return df.reset_index(drop=True)

# Fallback feature ranges for telemonitoring
FEATURE_META_REF = {
    f: {"lo": 0, "hi": 1} for f in [
        "MDVP:Fo(Hz)","MDVP:Fhi(Hz)","MDVP:Flo(Hz)",
        "MDVP:Jitter(%)","MDVP:Jitter(Abs)","MDVP:RAP","MDVP:PPQ",
        "Jitter:DDP","MDVP:Shimmer","MDVP:Shimmer(dB)",
        "Shimmer:APQ3","Shimmer:APQ5","MDVP:APQ","Shimmer:DDA",
        "NHR","HNR","RPDE","DFA","spread1","spread2","D2","PPE"
    ]
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SMOTE + SAMPLING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data
def apply_sampling(df_hash, _Xtr, _ytr):
    if not SMOTE_OK:
        return {"Original": (_Xtr, _ytr)}
    results = {"Original": (_Xtr, _ytr)}
    try:
        sm = SMOTE(random_state=42, k_neighbors=min(5, (_ytr==0).sum()-1))
        Xs, ys = sm.fit_resample(_Xtr, _ytr)
        results["SMOTE"] = (Xs, ys)
    except Exception:
        pass
    try:
        ada = ADASYN(random_state=42, n_neighbors=min(5, (_ytr==0).sum()-1))
        Xa, ya = ada.fit_resample(_Xtr, _ytr)
        results["ADASYN"] = (Xa, ya)
    except Exception:
        pass
    try:
        smt = SMOTETomek(random_state=42)
        Xst, yst = smt.fit_resample(_Xtr, _ytr)
        results["SMOTETomek"] = (Xst, yst)
    except Exception:
        pass
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  OPTUNA TUNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def optuna_tune_xgb(Xtr, ytr, n_trials=40):
    if not (XGB_OK and OPTUNA_OK):
        return None, None
    cv = StratifiedKFold(5, shuffle=True, random_state=42)

    def objective(trial):
        params = {
            "n_estimators":     trial.suggest_int("n_estimators", 100, 500),
            "max_depth":        trial.suggest_int("max_depth", 3, 9),
            "learning_rate":    trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "subsample":        trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha":        trial.suggest_float("reg_alpha", 1e-5, 10, log=True),
            "reg_lambda":       trial.suggest_float("reg_lambda", 1e-5, 10, log=True),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
            "eval_metric": "logloss", "use_label_encoder": False,
            "random_state": 42, "verbosity": 0,
        }
        clf = xgb.XGBClassifier(**params)
        scores = cross_val_score(clf, Xtr, ytr, cv=cv, scoring="roc_auc", n_jobs=-1)
        return scores.mean()

    study = optuna.create_study(direction="maximize",
                                sampler=optuna.samplers.TPESampler(seed=42))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
    best_params = study.best_params
    best_params.update({"eval_metric":"logloss","use_label_encoder":False,
                        "random_state":42,"verbosity":0})
    best_model = xgb.XGBClassifier(**best_params)
    best_model.fit(Xtr, ytr)
    return best_model, study


def optuna_tune_lgb(Xtr, ytr, n_trials=40):
    if not (LGB_OK and OPTUNA_OK):
        return None, None
    cv = StratifiedKFold(5, shuffle=True, random_state=42)

    def objective(trial):
        params = {
            "n_estimators":     trial.suggest_int("n_estimators", 100, 500),
            "max_depth":        trial.suggest_int("max_depth", 3, 9),
            "learning_rate":    trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "num_leaves":       trial.suggest_int("num_leaves", 20, 150),
            "subsample":        trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha":        trial.suggest_float("reg_alpha", 1e-5, 10, log=True),
            "reg_lambda":       trial.suggest_float("reg_lambda", 1e-5, 10, log=True),
            "min_child_samples":trial.suggest_int("min_child_samples", 5, 50),
            "random_state": 42, "verbosity": -1,
        }
        clf = lgb.LGBMClassifier(**params)
        scores = cross_val_score(clf, Xtr, ytr, cv=cv, scoring="roc_auc", n_jobs=-1)
        return scores.mean()

    study = optuna.create_study(direction="maximize",
                                sampler=optuna.samplers.TPESampler(seed=42))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
    best_params = study.best_params
    best_params.update({"random_state":42,"verbosity":-1})
    best_model = lgb.LGBMClassifier(**best_params)
    best_model.fit(Xtr, ytr)
    return best_model, study


def optuna_tune_cat(Xtr, ytr, n_trials=30):
    if not (CAT_OK and OPTUNA_OK):
        return None, None
    cv = StratifiedKFold(5, shuffle=True, random_state=42)

    def objective(trial):
        params = {
            "iterations":       trial.suggest_int("iterations", 100, 500),
            "depth":            trial.suggest_int("depth", 3, 10),
            "learning_rate":    trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "l2_leaf_reg":      trial.suggest_float("l2_leaf_reg", 1e-3, 10, log=True),
            "border_count":     trial.suggest_int("border_count", 32, 255),
            "random_strength":  trial.suggest_float("random_strength", 1e-3, 10, log=True),
            "random_state": 42, "verbose": 0,
        }
        clf = cb.CatBoostClassifier(**params)
        scores = cross_val_score(clf, Xtr, ytr, cv=cv, scoring="roc_auc", n_jobs=1)
        return scores.mean()

    study = optuna.create_study(direction="maximize",
                                sampler=optuna.samplers.TPESampler(seed=42))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
    best_params = study.best_params
    best_params.update({"random_state":42,"verbose":0})
    best_model = cb.CatBoostClassifier(**best_params)
    best_model.fit(Xtr, ytr)
    return best_model, study


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STACKING ENSEMBLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_resource
def build_stacking(df_hash, _Xtr, _ytr, _Xte, _yte, xgb_model, lgb_model, cat_model):
    estimators = [
        ("rf",  RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),
        ("gb",  GradientBoostingClassifier(n_estimators=150, learning_rate=0.08, random_state=42)),
        ("svm", SVC(kernel="rbf", C=10, gamma=0.01, probability=True, random_state=42)),
        ("knn", KNeighborsClassifier(n_neighbors=7)),
    ]
    if xgb_model:  estimators.append(("xgb", xgb_model))
    if lgb_model:  estimators.append(("lgb", lgb_model))
    if cat_model:  estimators.append(("cat", cat_model))

    # Meta-learner: Logistic Regression with L2
    stack = StackingClassifier(
        estimators=estimators,
        final_estimator=LogisticRegression(C=1.0, max_iter=1000, random_state=42),
        cv=StratifiedKFold(5, shuffle=True, random_state=42),
        passthrough=False, n_jobs=-1,
    )
    stack.fit(_Xtr, _ytr)
    return stack, get_full_metrics(stack, _Xte, _yte, "Stacking Ensemble")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STATISTICAL TESTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def mcnemar_test(y_true, pred1, pred2):
    """McNemar's test for comparing two classifiers."""
    b = np.sum((pred1 == y_true) & (pred2 != y_true))
    c = np.sum((pred1 != y_true) & (pred2 == y_true))
    if b + c == 0:
        return 1.0, 0.0
    chi2 = (abs(b - c) - 1)**2 / (b + c)
    p_val = 1 - stats.chi2.cdf(chi2, df=1)
    return p_val, chi2


def delong_auc_test(y_true, proba1, proba2):
    """DeLong's test for comparing two AUC values."""
    def compute_midrank(x):
        J = np.argsort(x)
        Z = x[J]
        N = len(x)
        T = np.zeros(N)
        i = 0
        while i < N:
            j = i
            while j < N and Z[j] == Z[i]:
                j += 1
            T[i:j] = 0.5 * (i + j - 1)
            i = j
        T2 = np.empty(N)
        T2[J] = T + 1
        return T2

    def fastDeLong(predictions_sorted_transposed, label_1_count):
        m = label_1_count
        n = predictions_sorted_transposed.shape[1] - m
        positive_examples = predictions_sorted_transposed[:, :m]
        negative_examples = predictions_sorted_transposed[:, m:]
        k = predictions_sorted_transposed.shape[0]
        tx = np.empty([k, m], dtype=float)
        ty = np.empty([k, n], dtype=float)
        tz = np.empty([k, m + n], dtype=float)
        for r in range(k):
            tx[r, :] = compute_midrank(positive_examples[r, :])
            ty[r, :] = compute_midrank(negative_examples[r, :])
            tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])
        aucs = (tz[:, :m].sum(axis=1) - tx.sum(axis=1)) / (m * n)
        v01 = (tz[:, :m] - tx[:, :]) / n
        v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m
        sx = np.cov(v01)
        sy = np.cov(v10)
        delongcov = sx / m + sy / n
        return aucs, delongcov

    y = np.array(y_true)
    p1 = np.array(proba1)
    p2 = np.array(proba2)
    sorted_idx = np.argsort(y)[::-1]
    y_sorted = y[sorted_idx]
    m = y_sorted.sum()
    preds = np.vstack([p1[sorted_idx], p2[sorted_idx]])
    aucs, cov = fastDeLong(preds, int(m))
    auc_diff = aucs[0] - aucs[1]
    var_diff = cov[0,0] + cov[1,1] - 2*cov[0,1]
    if var_diff <= 0:
        return aucs[0], aucs[1], 1.0, 0.0
    z = auc_diff / np.sqrt(var_diff)
    p_val = 2 * (1 - stats.norm.cdf(abs(z)))
    return aucs[0], aucs[1], p_val, z


def bootstrap_ci(y_true, y_proba, metric_fn, n_boot=1000, ci=0.95, seed=42):
    """Bootstrap confidence interval for any metric."""
    rng  = np.random.RandomState(seed)
    y    = np.array(y_true)
    p    = np.array(y_proba)
    boot = []
    for _ in range(n_boot):
        idx  = rng.choice(len(y), len(y), replace=True)
        if len(np.unique(y[idx])) < 2:
            continue
        try:
            boot.append(metric_fn(y[idx], p[idx]))
        except Exception:
            continue
    boot = np.array(boot)
    alpha = (1 - ci) / 2
    return (boot.mean(), boot.std(),
            np.percentile(boot, alpha*100),
            np.percentile(boot, (1-alpha)*100))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN RENDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def render_advanced_tabs(df, scaler, feat_names, existing_results):
    df_hash = len(df)
    X  = df[feat_names]; y = df["status"]
    Xs = scaler.transform(X)
    Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.2, stratify=y, random_state=42)

    adv_tabs = st.tabs([
        "ğŸš€ Boosting Models + Optuna",
        "ğŸ¯ Stacking Ensemble",
        "âš–ï¸ SMOTE + Class Balance",
        "ğŸ“ Calibration + Uncertainty",
        "ğŸ”¬ Statistical Tests",
        "ğŸŒ Cross-Dataset Validation",
    ])


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 0 â€” BOOSTING + OPTUNA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[0]:
        info_box("""<strong>XGBoost + LightGBM + CatBoost</strong> â€” the three industry-standard
            gradient boosting libraries used at Google, Meta, Kaggle competitions.
            <strong>Optuna</strong> performs Bayesian hyperparameter optimisation (TPE sampler)
            â€” smarter than GridSearchCV, finds better params with fewer trials.""")

        missing = []
        if not XGB_OK:   missing.append("`pip install xgboost`")
        if not LGB_OK:   missing.append("`pip install lightgbm`")
        if not CAT_OK:   missing.append("`pip install catboost`")
        if not OPTUNA_OK:missing.append("`pip install optuna`")
        if missing:
            warn_box(f"Missing packages: {' Â· '.join(missing)}")

        boost_results = {}

        # â”€â”€ XGBoost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        section("XGBoost â€” Optuna Tuning")
        if XGB_OK and OPTUNA_OK:
            with st.spinner("Tuning XGBoost with Optuna (40 trials Ã— 5-fold CV)â€¦"):
                @st.cache_resource
                def _xgb(dh, _Xtr, _ytr): return optuna_tune_xgb(_Xtr, _ytr, n_trials=40)
                xgb_model, xgb_study = _xgb(df_hash, Xtr, ytr)
            boost_results["XGBoost"] = get_full_metrics(xgb_model, Xte, yte, "XGBoost")
            xm = boost_results["XGBoost"]

            c1,c2,c3,c4 = st.columns(4)
            for col,(lbl,val,color) in zip([c1,c2,c3,c4],[
                ("Test AUC",      f"{xm['roc_auc']:.4f}",         BLUE),
                ("Accuracy",      f"{xm['accuracy']*100:.1f}%",   GREEN),
                ("F1",            f"{xm['f1']:.4f}",               AMBER),
                ("Best Trial AUC",f"{xgb_study.best_value:.4f}", PURPLE),
            ]):
                with col: metric_card(val, lbl, color)

            st.markdown("<br>", unsafe_allow_html=True)
            col1, col2 = st.columns(2)
            with col1:
                section("Optuna Optimization History")
                trial_vals = [t.value for t in xgb_study.trials if t.value is not None]
                best_so_far = np.maximum.accumulate(trial_vals)
                fig, ax = plt.subplots(figsize=(5, 3.5))
                ax.scatter(range(len(trial_vals)), trial_vals, color=BLUE, alpha=0.5, s=20, label="Trial AUC")
                ax.plot(range(len(best_so_far)), best_so_far, color=RED, lw=2, label="Best so far")
                ax.set(title="Optuna â€” XGBoost Trial History", xlabel="Trial", ylabel="CV ROC AUC")
                ax.legend(); ax.grid(True)
                fig.tight_layout(); st.pyplot(fig); plt.close(fig)
            with col2:
                section("Best Hyperparameters")
                bp = xgb_study.best_params
                for k, v in bp.items():
                    st.markdown(f"""<div style='display:flex;justify-content:space-between;
                        font-size:0.82rem;padding:5px 0;border-bottom:1px solid {BORDER}'>
                        <span style='color:{TEXT_MID}'>{k}</span>
                        <span style='color:{NAVY};font-weight:600'>{v:.4f if isinstance(v,float) else v}</span>
                    </div>""", unsafe_allow_html=True)

            # Feature importance
            section("XGBoost Feature Importance")
            fi   = xgb_model.feature_importances_
            fi_idx = np.argsort(fi)[::-1][:15]
            fig, ax = plt.subplots(figsize=(10, 3.5))
            ax.bar([feat_names[i] for i in fi_idx], fi[fi_idx],
                   color=BLUE, alpha=0.8, edgecolor=WHITE)
            ax.set_xticklabels([feat_names[i] for i in fi_idx], rotation=45, ha="right", fontsize=8)
            ax.set(title="XGBoost Feature Importance (Gain)", ylabel="Importance")
            ax.grid(axis="y"); fig.tight_layout(); st.pyplot(fig); plt.close(fig)
        else:
            warn_box("Install xgboost and optuna to enable this section.")
            xgb_model = None

        # â”€â”€ LightGBM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        section("LightGBM â€” Optuna Tuning")
        if LGB_OK and OPTUNA_OK:
            with st.spinner("Tuning LightGBM with Optuna (40 trials)â€¦"):
                @st.cache_resource
                def _lgb(dh, _Xtr, _ytr): return optuna_tune_lgb(_Xtr, _ytr, n_trials=40)
                lgb_model, lgb_study = _lgb(df_hash, Xtr, ytr)
            boost_results["LightGBM"] = get_full_metrics(lgb_model, Xte, yte, "LightGBM")
            lm = boost_results["LightGBM"]

            c1,c2,c3,c4 = st.columns(4)
            for col,(lbl,val,color) in zip([c1,c2,c3,c4],[
                ("Test AUC",      f"{lm['roc_auc']:.4f}",         BLUE),
                ("Accuracy",      f"{lm['accuracy']*100:.1f}%",   GREEN),
                ("F1",            f"{lm['f1']:.4f}",               AMBER),
                ("Best Trial AUC",f"{lgb_study.best_value:.4f}", PURPLE),
            ]):
                with col: metric_card(val, lbl, color)

            col1, col2 = st.columns(2)
            with col1:
                section("Optuna History â€” LightGBM")
                trial_vals2 = [t.value for t in lgb_study.trials if t.value is not None]
                best2 = np.maximum.accumulate(trial_vals2)
                fig, ax = plt.subplots(figsize=(5, 3.5))
                ax.scatter(range(len(trial_vals2)), trial_vals2, color=GREEN, alpha=0.5, s=20)
                ax.plot(range(len(best2)), best2, color=RED, lw=2, label="Best so far")
                ax.set(title="Optuna â€” LightGBM Trial History", xlabel="Trial", ylabel="CV ROC AUC")
                ax.legend(); ax.grid(True)
                fig.tight_layout(); st.pyplot(fig); plt.close(fig)
            with col2:
                section("LightGBM Feature Importance")
                fi_lgb = lgb_model.feature_importances_
                fi_lgb_idx = np.argsort(fi_lgb)[::-1][:10]
                fig, ax = plt.subplots(figsize=(5, 3.5))
                ax.barh([feat_names[i] for i in fi_lgb_idx[::-1]], fi_lgb[fi_lgb_idx[::-1]],
                        color=GREEN, alpha=0.8, edgecolor=WHITE)
                ax.set(title="LightGBM Feature Importance", xlabel="Importance")
                ax.grid(axis="x"); fig.tight_layout(); st.pyplot(fig); plt.close(fig)
        else:
            warn_box("Install lightgbm and optuna to enable this section.")
            lgb_model = None

        # â”€â”€ CatBoost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        section("CatBoost â€” Optuna Tuning")
        if CAT_OK and OPTUNA_OK:
            with st.spinner("Tuning CatBoost with Optuna (30 trials)â€¦"):
                @st.cache_resource
                def _cat(dh, _Xtr, _ytr): return optuna_tune_cat(_Xtr, _ytr, n_trials=30)
                cat_model, cat_study = _cat(df_hash, Xtr, ytr)
            boost_results["CatBoost"] = get_full_metrics(cat_model, Xte, yte, "CatBoost")
            cm_ = boost_results["CatBoost"]

            c1,c2,c3,c4 = st.columns(4)
            for col,(lbl,val,color) in zip([c1,c2,c3,c4],[
                ("Test AUC",      f"{cm_['roc_auc']:.4f}",          BLUE),
                ("Accuracy",      f"{cm_['accuracy']*100:.1f}%",    GREEN),
                ("F1",            f"{cm_['f1']:.4f}",                AMBER),
                ("Best Trial AUC",f"{cat_study.best_value:.4f}",   PURPLE),
            ]):
                with col: metric_card(val, lbl, color)
        else:
            warn_box("Install catboost and optuna to enable this section.")
            cat_model = None

        # â”€â”€ Boosting comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if boost_results:
            section("Boosting Models â€” ROC Comparison")
            fig, ax = plt.subplots(figsize=(8, 4.5))
            boost_pal = [BLUE, GREEN, AMBER]
            for (name, res), c in zip(boost_results.items(), boost_pal):
                ax.plot(res["fpr"], res["tpr"], color=c, lw=2.5,
                        label=f"{name} (AUC={res['roc_auc']:.4f})")
                ax.fill_between(res["fpr"], res["tpr"], alpha=0.06, color=c)
            # Also plot best classical
            best_classical = max(existing_results, key=lambda k: existing_results[k]["roc_auc"])
            bc = existing_results[best_classical]
            ax.plot(bc["fpr"], bc["tpr"], color=TEXT_DIM, lw=1.5, ls="--",
                    label=f"Best Classical: {best_classical} ({bc['roc_auc']:.4f})")
            ax.plot([0,1],[0,1], color=BORDER, lw=1, ls=":")
            ax.set(title="Boosting Models vs Best Classical â€” ROC Curves", xlabel="FPR", ylabel="TPR")
            ax.legend(fontsize=8); ax.grid(True)
            fig.tight_layout(); st.pyplot(fig); plt.close(fig)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 1 â€” STACKING ENSEMBLE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[1]:
        info_box("""<strong>Stacking Ensemble</strong> trains a <em>meta-learner</em>
            (Logistic Regression) on the out-of-fold predictions of all base models.
            The meta-learner learns which classifiers to trust for which inputs â€”
            often outperforming any individual model. This is what winning Kaggle teams use.""")

        xgb_m = None; lgb_m = None; cat_m = None
        try:
            if XGB_OK and OPTUNA_OK:
                @st.cache_resource
                def _xgb2(dh, _Xtr, _ytr): return optuna_tune_xgb(_Xtr, _ytr, n_trials=40)
                xgb_m, _ = _xgb2(df_hash, Xtr, ytr)
        except Exception: pass
        try:
            if LGB_OK and OPTUNA_OK:
                @st.cache_resource
                def _lgb2(dh, _Xtr, _ytr): return optuna_tune_lgb(_Xtr, _ytr, n_trials=40)
                lgb_m, _ = _lgb2(df_hash, Xtr, ytr)
        except Exception: pass
        try:
            if CAT_OK and OPTUNA_OK:
                @st.cache_resource
                def _cat2(dh, _Xtr, _ytr): return optuna_tune_cat(_Xtr, _ytr, n_trials=30)
                cat_m, _ = _cat2(df_hash, Xtr, ytr)
        except Exception: pass

        with st.spinner("Building Stacking Ensemble (this trains all base models with 5-fold OOF)â€¦"):
            stack_clf, stack_metrics = build_stacking(df_hash, Xtr, ytr, Xte, yte, xgb_m, lgb_m, cat_m)

        sm = stack_metrics
        c1,c2,c3,c4,c5 = st.columns(5)
        for col,(lbl,val,color) in zip([c1,c2,c3,c4,c5],[
            ("ROC AUC",   f"{sm['roc_auc']:.4f}",        BLUE),
            ("Accuracy",  f"{sm['accuracy']*100:.1f}%",  GREEN),
            ("F1 Score",  f"{sm['f1']:.4f}",              AMBER),
            ("MCC",       f"{sm['mcc']:.4f}",             PURPLE),
            ("Brier",     f"{sm['brier']:.4f}",           RED),
        ]):
            with col: metric_card(val, lbl, color)

        st.markdown("<br>", unsafe_allow_html=True)
        col1, col2 = st.columns(2)
        with col1:
            section("Stacking Ensemble â€” Confusion Matrix")
            fig, ax = plt.subplots(figsize=(5, 4))
            cm_n = sm["cm"].astype(float)/sm["cm"].sum(axis=1,keepdims=True)
            im=ax.imshow(cm_n,cmap=LinearSegmentedColormap.from_list("cm",[WHITE,BLUE_LIGHT,BLUE],256),vmin=0,vmax=1)
            for i in range(2):
                for j in range(2):
                    ax.text(j,i,f"{sm['cm'][i,j]}\n({cm_n[i,j]*100:.1f}%)",
                            ha="center",va="center",fontsize=13,fontweight="bold",
                            color=WHITE if cm_n[i,j]>0.6 else NAVY)
            ax.set_xticks([0,1]); ax.set_yticks([0,1])
            ax.set_xticklabels(["Pred: Healthy","Pred: PD"])
            ax.set_yticklabels(["True: Healthy","True: PD"])
            ax.set_title("Stacking Ensemble â€” Confusion Matrix")
            plt.colorbar(im,ax=ax,fraction=0.046)
            fig.tight_layout(); st.pyplot(fig); plt.close(fig)

        with col2:
            section("Stacking vs All Models â€” ROC")
            fig, ax = plt.subplots(figsize=(5, 4))
            for (name,res), c in zip(list(existing_results.items())[:5], PAL):
                ax.plot(res["fpr"],res["tpr"],color=c,lw=1.2,alpha=0.5,label=f"{name} ({res['roc_auc']:.3f})")
            ax.plot(sm["fpr"],sm["tpr"],color=NAVY,lw=3,label=f"Stacking ({sm['roc_auc']:.4f}) â˜…")
            ax.plot([0,1],[0,1],color=BORDER,lw=1,ls="--")
            ax.set(title="ROC â€” Stacking vs Base Models",xlabel="FPR",ylabel="TPR")
            ax.legend(fontsize=7,loc="lower right"); ax.grid(True)
            fig.tight_layout(); st.pyplot(fig); plt.close(fig)

        section("Classification Report")
        st.code(sm["report"], language="")

        # Meta-learner coefficients
        section("Meta-Learner Coefficients (What the Stacker Learned)")
        try:
            meta = stack_clf.final_estimator_
            base_names = [name for name, _ in stack_clf.estimators]
            coefs = meta.coef_[0]
            fig, ax = plt.subplots(figsize=(9, 3))
            colors_coef = [GREEN if c > 0 else RED for c in coefs]
            ax.bar(base_names[:len(coefs)], coefs, color=colors_coef, alpha=0.8, edgecolor=WHITE)
            ax.axhline(0, color=NAVY, lw=1.5)
            ax.set(title="Meta-Learner Coefficients â€” Which base models the stacker trusts most",
                   ylabel="Logistic Regression Coefficient")
            ax.set_xticklabels(base_names[:len(coefs)], rotation=30, ha="right", fontsize=8)
            ax.grid(axis="y"); fig.tight_layout(); st.pyplot(fig); plt.close(fig)
        except Exception as e:
            st.info(f"Meta-learner coef plot skipped: {e}")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 2 â€” SMOTE + CLASS BALANCE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[2]:
        info_box("""<strong>Class Imbalance</strong> (75% PD, 25% Healthy) can bias models
            toward the majority class. <strong>SMOTE</strong> (Synthetic Minority Oversampling),
            <strong>ADASYN</strong>, and <strong>SMOTETomek</strong> create synthetic minority
            samples to balance training data. We compare their impact on model performance.""")

        if not SMOTE_OK:
            warn_box("Install imbalanced-learn: `pip install imbalanced-learn`")
        else:
            with st.spinner("Applying SMOTE, ADASYN, SMOTETomek and re-trainingâ€¦"):
                @st.cache_data
                def _sampling(dh, _Xtr, _ytr): return apply_sampling(dh, _Xtr, _ytr)
                sampling_results = _sampling(df_hash, Xtr, ytr)

            # Train RF on each sampled dataset
            smote_metrics = {}
            for method, (Xs, ys) in sampling_results.items():
                clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
                clf.fit(Xs, ys)
                smote_metrics[method] = get_full_metrics(clf, Xte, yte, method)

            # Class distribution comparison
            section("Class Distribution Before / After Resampling")
            fig, axes = plt.subplots(1, len(sampling_results), figsize=(4*len(sampling_results), 3.5))
            if len(sampling_results) == 1: axes = [axes]
            for ax, (method, (Xs, ys)) in zip(axes, sampling_results.items()):
                counts = pd.Series(ys).value_counts().sort_index()
                ax.bar(["Healthy","PD"], [counts.get(0,0), counts.get(1,0)],
                       color=[GREEN, RED], alpha=0.8, edgecolor=WHITE)
                ax.set_title(f"{method}\nn={len(ys)}", fontsize=9, fontweight="bold")
                ax.set_ylabel("Count"); ax.grid(axis="y")
                for i, v in enumerate([counts.get(0,0), counts.get(1,0)]):
                    ax.text(i, v+2, str(v), ha="center", fontsize=9, fontweight="bold")
            fig.suptitle("Training Set Class Balance After Resampling", color=NAVY, fontweight="bold")
            fig.tight_layout(); st.pyplot(fig); plt.close(fig)

            # Performance comparison
            section("Impact on Model Performance")
            c1,c2 = st.columns(2)
            with c1:
                comp_rows = []
                for method, m in smote_metrics.items():
                    comp_rows.append({
                        "Method": method,
                        "Accuracy": f"{m['accuracy']*100:.2f}%",
                        "F1": f"{m['f1']:.4f}",
                        "ROC AUC": f"{m['roc_auc']:.4f}",
                        "Recall": f"{m['recall']:.4f}",
                        "Precision": f"{m['precision']:.4f}",
                        "Brier": f"{m['brier']:.4f}",
                    })
                st.dataframe(pd.DataFrame(comp_rows), use_container_width=True)

            with c2:
                fig, ax = plt.subplots(figsize=(5, 4))
                methods = list(smote_metrics.keys())
                for m_name, res, c in zip(methods, smote_metrics.values(), PAL):
                    ax.plot(res["fpr"], res["tpr"], color=c, lw=2.5,
                            label=f"{m_name} (AUC={res['roc_auc']:.4f})")
                ax.plot([0,1],[0,1],color=BORDER,lw=1,ls="--")
                ax.set(title="SMOTE Methods â€” ROC Comparison",xlabel="FPR",ylabel="TPR")
                ax.legend(fontsize=7.5); ax.grid(True)
                fig.tight_layout(); st.pyplot(fig); plt.close(fig)

            # Recall vs Precision tradeoff
            section("Recall vs Precision â€” Clinical Impact")
            info_box("""In clinical settings, <strong>Recall (Sensitivity)</strong> for PD is
                critical â€” missing a PD patient (false negative) is more harmful than a
                false alarm. SMOTE typically improves recall at the cost of some precision.""")
            fig, ax = plt.subplots(figsize=(9, 3.5))
            x = np.arange(len(methods)); w = 0.25
            recalls    = [smote_metrics[m]["recall"]    for m in methods]
            precisions = [smote_metrics[m]["precision"] for m in methods]
            f1s        = [smote_metrics[m]["f1"]        for m in methods]
            ax.bar(x-w, recalls,    w, label="Recall",    color=GREEN, alpha=0.8, edgecolor=WHITE)
            ax.bar(x,   precisions, w, label="Precision", color=BLUE,  alpha=0.8, edgecolor=WHITE)
            ax.bar(x+w, f1s,        w, label="F1",        color=AMBER, alpha=0.8, edgecolor=WHITE)
            ax.set_xticks(x); ax.set_xticklabels(methods)
            ax.set(title="Recall / Precision / F1 per Resampling Method", ylabel="Score", ylim=[0,1.1])
            ax.legend(); ax.grid(axis="y"); fig.tight_layout(); st.pyplot(fig); plt.close(fig)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 3 â€” CALIBRATION + UNCERTAINTY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[3]:
        info_box("""<strong>Calibration</strong> measures whether predicted probabilities
            are reliable â€” if a model says 80% PD, does 80% of that group actually have PD?
            <strong>Uncertainty Quantification</strong> via Bootstrap tells us how confident
            we should be in our AUC estimates. Both are required in real clinical ML systems.""")

        section("Calibration Curves â€” Reliability Diagrams")
        # Calibrate top 4 models
        top_models = sorted(existing_results.items(), key=lambda x: -x[1]["roc_auc"])[:4]
        fig, axes = plt.subplots(2, 2, figsize=(12, 9))
        axes = axes.flatten()
        for ax, (name, res) in zip(axes, top_models):
            # Original
            prob_true_orig, prob_pred_orig = calibration_curve(
                res["y_test"], res["y_proba"], n_bins=8, strategy="uniform"
            )
            ax.plot(prob_pred_orig, prob_true_orig, "s-", color=BLUE, lw=2,
                    label=f"Original (Brier={brier_score_loss(res['y_test'],res['y_proba']):.4f})")

            # Isotonic calibrated
            try:
                cal_iso = CalibratedClassifierCV(res["clf"], method="isotonic", cv=5)
                cal_iso.fit(Xtr, ytr)
                iso_proba = cal_iso.predict_proba(Xte)[:,1]
                pt_iso, pp_iso = calibration_curve(yte, iso_proba, n_bins=8, strategy="uniform")
                ax.plot(pp_iso, pt_iso, "o-", color=GREEN, lw=2,
                        label=f"Isotonic (Brier={brier_score_loss(yte,iso_proba):.4f})")
            except Exception: pass

            # Platt scaling
            try:
                cal_sig = CalibratedClassifierCV(res["clf"], method="sigmoid", cv=5)
                cal_sig.fit(Xtr, ytr)
                sig_proba = cal_sig.predict_proba(Xte)[:,1]
                pt_sig, pp_sig = calibration_curve(yte, sig_proba, n_bins=8, strategy="uniform")
                ax.plot(pp_sig, pt_sig, "^-", color=AMBER, lw=2,
                        label=f"Platt (Brier={brier_score_loss(yte,sig_proba):.4f})")
            except Exception: pass

            ax.plot([0,1],[0,1], color=BORDER_MED, lw=1.5, ls="--", label="Perfect calibration")
            ax.fill_between([0,1],[0,1],[0,1], alpha=0.05, color=GREEN)
            ax.set(title=name, xlabel="Mean Predicted Probability", ylabel="Fraction of Positives")
            ax.legend(fontsize=7); ax.grid(True)

        fig.suptitle("Calibration Curves â€” Original vs Isotonic vs Platt Scaling",
                     y=1.01, color=NAVY, fontsize=12, fontweight="bold")
        fig.tight_layout(); st.pyplot(fig); plt.close(fig)

        section("Bootstrap Confidence Intervals â€” AUC")
        info_box("""Bootstrap CI shows the range of AUC values we'd expect if we retrained
            on different random samples of the same population.
            Narrow CI = stable model. Wide CI = model is sensitive to data variation.""")

        best_models_ci = sorted(existing_results.items(), key=lambda x: -x[1]["roc_auc"])[:6]
        ci_rows = []
        with st.spinner("Computing 1000-sample Bootstrap CIsâ€¦"):
            for name, res in best_models_ci:
                def auc_fn(y, p):
                    fpr_, tpr_, _ = roc_curve(y, p)
                    return auc(fpr_, tpr_)
                mean_auc, std_auc, lo, hi = bootstrap_ci(res["y_test"], res["y_proba"], auc_fn, n_boot=1000)
                ci_rows.append({"Model": name, "AUC": f"{res['roc_auc']:.4f}",
                                "Boot Mean": f"{mean_auc:.4f}", "Boot Std": f"{std_auc:.4f}",
                                "95% CI Lower": f"{lo:.4f}", "95% CI Upper": f"{hi:.4f}",
                                "CI Width": f"{hi-lo:.4f}"})

        ci_df = pd.DataFrame(ci_rows)
        st.dataframe(ci_df, use_container_width=True)

        # Forest plot
        fig, ax = plt.subplots(figsize=(9, 4))
        for i, row in enumerate(ci_rows):
            m    = float(row["AUC"])
            lo_  = float(row["95% CI Lower"])
            hi_  = float(row["95% CI Upper"])
            ax.plot([lo_, hi_], [i, i], color=BLUE, lw=3, solid_capstyle="round")
            ax.scatter(m, i, color=NAVY, s=60, zorder=5)
            ax.text(hi_+0.003, i, f"{m:.4f} [{lo_:.3f}â€“{hi_:.3f}]",
                    va="center", fontsize=7.5, color=TEXT_MID)
        ax.set_yticks(range(len(ci_rows)))
        ax.set_yticklabels([r["Model"] for r in ci_rows])
        ax.axvline(0.9, color=AMBER, lw=1, ls="--", alpha=0.7, label="AUC=0.90")
        ax.axvline(1.0, color=GREEN, lw=1, ls="--", alpha=0.7, label="Perfect=1.0")
        ax.set(title="Forest Plot â€” AUC with 95% Bootstrap CI", xlabel="ROC AUC")
        ax.legend(fontsize=8); ax.grid(axis="x")
        fig.tight_layout(); st.pyplot(fig); plt.close(fig)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 4 â€” STATISTICAL TESTS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[4]:
        info_box("""<strong>Statistical significance testing</strong> answers: are these model
            differences real, or just random variation? <br>
            â€¢ <strong>McNemar's Test</strong> â€” are two classifiers' error patterns significantly different?<br>
            â€¢ <strong>DeLong's Test</strong> â€” is the AUC difference between two models significant?<br>
            â€¢ <strong>Wilcoxon Signed-Rank</strong> â€” non-parametric CV score comparison""")

        model_names = list(existing_results.keys())
        n_models    = len(model_names)

        # McNemar p-value matrix
        section("McNemar's Test â€” Pairwise Classifier Comparison")
        st.markdown(f"<div style='font-size:0.8rem;color:{TEXT_MID};margin-bottom:12px'>p-values for pairwise McNemar's test. <strong style='color:{RED}'>Red</strong> = significant (p&lt;0.05), meaning the two models make meaningfully different errors.</div>", unsafe_allow_html=True)

        mcnemar_matrix = np.ones((n_models, n_models))
        chi2_matrix    = np.zeros((n_models, n_models))
        for i, n1 in enumerate(model_names):
            for j, n2 in enumerate(model_names):
                if i != j:
                    p, c = mcnemar_test(
                        existing_results[n1]["y_test"],
                        existing_results[n1]["y_pred"],
                        existing_results[n2]["y_pred"],
                    )
                    mcnemar_matrix[i,j] = p
                    chi2_matrix[i,j]    = c

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))
        sig_cmap = LinearSegmentedColormap.from_list("sig",[RED, AMBER_LIGHT, WHITE], 256)
        im1 = ax1.imshow(mcnemar_matrix, cmap=sig_cmap, vmin=0, vmax=0.2)
        for i in range(n_models):
            for j in range(n_models):
                val = mcnemar_matrix[i,j]
                sig = "***" if val<0.001 else ("**" if val<0.01 else ("*" if val<0.05 else "ns"))
                txt = f"{val:.3f}\n{sig}" if i!=j else "â€”"
                ax1.text(j, i, txt, ha="center", va="center", fontsize=6.5,
                         fontweight="bold" if val<0.05 else "normal",
                         color=WHITE if val<0.05 else TEXT_MAIN)
        ax1.set_xticks(range(n_models)); ax1.set_yticks(range(n_models))
        ax1.set_xticklabels([n.replace(" ","\n") for n in model_names], fontsize=7)
        ax1.set_yticklabels(model_names, fontsize=7)
        ax1.set_title("McNemar p-values (lower = more different)")
        plt.colorbar(im1, ax=ax1, label="p-value", shrink=0.7)

        # DeLong AUC matrix
        delong_matrix = np.zeros((n_models, n_models))
        delong_p      = np.ones((n_models, n_models))
        for i, n1 in enumerate(model_names):
            for j, n2 in enumerate(model_names):
                if i != j:
                    try:
                        _, _, p, z = delong_auc_test(
                            existing_results[n1]["y_test"],
                            existing_results[n1]["y_proba"],
                            existing_results[n2]["y_proba"],
                        )
                        delong_p[i,j] = p
                    except Exception:
                        delong_p[i,j] = 1.0

        im2 = ax2.imshow(delong_p, cmap=sig_cmap, vmin=0, vmax=0.2)
        for i in range(n_models):
            for j in range(n_models):
                val = delong_p[i,j]
                sig = "***" if val<0.001 else ("**" if val<0.01 else ("*" if val<0.05 else "ns"))
                txt = f"{val:.3f}\n{sig}" if i!=j else "â€”"
                ax2.text(j, i, txt, ha="center", va="center", fontsize=6.5,
                         fontweight="bold" if val<0.05 else "normal",
                         color=WHITE if val<0.05 else TEXT_MAIN)
        ax2.set_xticks(range(n_models)); ax2.set_yticks(range(n_models))
        ax2.set_xticklabels([n.replace(" ","\n") for n in model_names], fontsize=7)
        ax2.set_yticklabels(model_names, fontsize=7)
        ax2.set_title("DeLong AUC Test p-values")
        plt.colorbar(im2, ax=ax2, label="p-value", shrink=0.7)

        fig.suptitle("Pairwise Statistical Tests â€” * p<0.05  ** p<0.01  *** p<0.001",
                     y=1.01, color=NAVY, fontsize=11, fontweight="bold")
        fig.tight_layout(); st.pyplot(fig); plt.close(fig)

        # Wilcoxon signed-rank on CV scores
        section("Wilcoxon Signed-Rank Test â€” CV Score Distributions")
        info_box("Non-parametric test comparing 10-fold CV accuracy distributions. Doesn't assume normality â€” more reliable than t-test for small samples.")

        best_m = max(model_names, key=lambda n: existing_results[n]["roc_auc"])
        wilcox_rows = []
        for name in model_names:
            if name == best_m: continue
            cv = StratifiedKFold(10, shuffle=True, random_state=42)
            Xs_all = scaler.transform(df[feat_names])
            s1 = cross_val_score(existing_results[best_m]["clf"], Xs_all, df["status"],
                                 cv=cv, scoring="accuracy")
            s2 = cross_val_score(existing_results[name]["clf"],   Xs_all, df["status"],
                                 cv=cv, scoring="accuracy")
            try:
                stat, p = stats.wilcoxon(s1, s2)
            except Exception:
                stat, p = 0, 1.0
            wilcox_rows.append({
                "Model A (best)": best_m,
                "Model B":        name,
                "A mean CV":      f"{s1.mean()*100:.2f}%",
                "B mean CV":      f"{s2.mean()*100:.2f}%",
                "W statistic":    f"{stat:.1f}",
                "p-value":        f"{p:.4f}",
                "Significant":    "âœ… Yes" if p < 0.05 else "âŒ No",
            })
        if wilcox_rows:
            st.dataframe(pd.DataFrame(wilcox_rows), use_container_width=True)

        # Effect size summary
        section("Effect Sizes Summary (Cohen's d on CV Scores)")
        fig, ax = plt.subplots(figsize=(10, 3.5))
        Xs_all = scaler.transform(df[feat_names])
        cv10 = StratifiedKFold(10, shuffle=True, random_state=42)
        all_cv = {}
        for name, res in existing_results.items():
            all_cv[name] = cross_val_score(res["clf"], Xs_all, df["status"], cv=cv10, scoring="accuracy")

        best_cv = all_cv[best_m]
        d_vals, names_d = [], []
        for name, cv_s in all_cv.items():
            if name == best_m: continue
            d = (best_cv.mean()-cv_s.mean())/np.sqrt((best_cv.std()**2+cv_s.std()**2)/2)
            d_vals.append(d); names_d.append(name)
        colors_d = [GREEN if d > 0 else RED for d in d_vals]
        ax.barh(names_d, d_vals, color=colors_d, alpha=0.8, edgecolor=WHITE)
        ax.axvline(0, color=NAVY, lw=1.5)
        ax.axvline(0.5, color=AMBER, lw=1, ls="--", alpha=0.7, label="Medium effect (0.5)")
        ax.axvline(-0.5, color=AMBER, lw=1, ls="--", alpha=0.7)
        ax.set(title=f"Cohen's d Effect Size vs {best_m} (positive = best model is better)",
               xlabel="Cohen's d")
        ax.legend(fontsize=8); ax.grid(axis="x")
        fig.tight_layout(); st.pyplot(fig); plt.close(fig)


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  TAB 5 â€” CROSS-DATASET VALIDATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with adv_tabs[5]:
        info_box("""<strong>Cross-Dataset Validation</strong> is the gold standard for
            evaluating generalizability. We train on the UCI Voice Dataset and test on a
            separate Telemonitoring dataset (different recording sessions, different patients).
            If a model performs well across both â€” it has learned real biomarker patterns,
            not just memorised the training set. This is what clinical ML validation looks like.""")

        warn_box("""For production: download real datasets from UCI ML Repository.
            <br>â€¢ Voice: archive.ics.uci.edu/dataset/174/parkinsons
            <br>â€¢ Telemonitoring: archive.ics.uci.edu/dataset/189/parkinsons+telemonitoring""")

        with st.spinner("Loading telemonitoring dataset and running cross-dataset validationâ€¦"):
            @st.cache_data
            def _tel(fn, _feat_names):
                try:
                    return load_telemonitoring(_feat_names, seed=42)
                except Exception:
                    return None
            df_tel = _tel(tuple(feat_names), feat_names)

        if df_tel is not None:
            n_tel = len(df_tel)
            n_tel_pd = int((df_tel.status==1).sum())
            n_tel_h  = int((df_tel.status==0).sum())

            col1, col2, col3, col4 = st.columns(4)
            with col1: metric_card(str(n_tel), "Telemonitoring\nSamples", BLUE)
            with col2: metric_card(str(n_tel_pd), "PD Recordings", RED)
            with col3: metric_card(str(n_tel_h), "Healthy Recordings", GREEN)
            with col4: metric_card("22", "Shared Features", AMBER)

            st.markdown("<br>", unsafe_allow_html=True)

            # Cross-dataset evaluation
            X_tel = df_tel[feat_names]
            y_tel = df_tel["status"]
            X_tel_scaled = scaler.transform(X_tel)  # Use SAME scaler as voice training

            section("Train on Voice Dataset â†’ Test on Telemonitoring Dataset")
            cross_rows = []
            best_cls = sorted(existing_results.items(), key=lambda x: -x[1]["roc_auc"])[:6]
            cross_results = {}
            for name, res in best_cls:
                yp_tel  = res["clf"].predict(X_tel_scaled)
                ypr_tel = res["clf"].predict_proba(X_tel_scaled)[:,1]
                fpr_tel, tpr_tel, _ = roc_curve(y_tel, ypr_tel)
                auc_tel = auc(fpr_tel, tpr_tel)
                acc_tel = accuracy_score(y_tel, yp_tel)
                f1_tel  = f1_score(y_tel, yp_tel)
                cross_results[name] = {
                    "fpr": fpr_tel, "tpr": tpr_tel,
                    "roc_auc_tel": auc_tel, "accuracy_tel": acc_tel, "f1_tel": f1_tel,
                    "roc_auc_orig": res["roc_auc"],
                }
                drop_auc = res["roc_auc"] - auc_tel
                cross_rows.append({
                    "Model":              name,
                    "Voice AUC (train)":  f"{res['roc_auc']:.4f}",
                    "Telemon AUC (test)": f"{auc_tel:.4f}",
                    "AUC Drop":           f"{drop_auc:+.4f}",
                    "Telemon Accuracy":   f"{acc_tel*100:.2f}%",
                    "Telemon F1":         f"{f1_tel:.4f}",
                    "Generalizes?":       "âœ… Well" if drop_auc < 0.05 else ("âš ï¸ Moderate" if drop_auc < 0.15 else "âŒ Poor"),
                })

            cross_df = pd.DataFrame(cross_rows).sort_values("Telemon AUC (test)", ascending=False).reset_index(drop=True)
            cross_df.index += 1
            st.dataframe(cross_df, use_container_width=True)

            # ROC comparison
            col1, col2 = st.columns(2)
            with col1:
                section("Cross-Dataset ROC Curves")
                fig, ax = plt.subplots(figsize=(6, 5))
                for (name, cr), c in zip(cross_results.items(), PAL):
                    ax.plot(cr["fpr"], cr["tpr"], color=c, lw=2,
                            label=f"{name[:20]} ({cr['roc_auc_tel']:.3f})")
                ax.plot([0,1],[0,1],color=BORDER,lw=1,ls="--")
                ax.set(title="Telemonitoring Test Set ROC\n(Models trained on Voice Dataset)",
                       xlabel="FPR",ylabel="TPR")
                ax.legend(fontsize=7,loc="lower right"); ax.grid(True)
                fig.tight_layout(); st.pyplot(fig); plt.close(fig)

            with col2:
                section("Voice AUC vs Telemonitoring AUC")
                fig, ax = plt.subplots(figsize=(6, 5))
                for (name, cr), c in zip(cross_results.items(), PAL):
                    ax.scatter(cr["roc_auc_orig"], cr["roc_auc_tel"],
                               color=c, s=120, edgecolors=BORDER_MED, lw=1.5, zorder=4)
                    ax.annotate(name, (cr["roc_auc_orig"], cr["roc_auc_tel"]),
                                textcoords="offset points", xytext=(6,4), fontsize=7, color=c)
                ax.plot([0.7,1.0],[0.7,1.0], color=BORDER_MED, lw=1, ls="--",
                        label="Perfect generalization (no drop)")
                ax.fill_between([0.7,1.0],[0.65,0.95],[0.7,1.0], alpha=0.05, color=GREEN)
                ax.set(title="Generalization Gap", xlabel="Voice Dataset AUC",
                       ylabel="Telemonitoring AUC", xlim=[0.75,1.02], ylim=[0.65,1.02])
                ax.legend(fontsize=8); ax.grid(True)
                fig.tight_layout(); st.pyplot(fig); plt.close(fig)

            # AUC drop bar chart
            section("AUC Generalization Gap â€” How Much Performance Drops")
            fig, ax = plt.subplots(figsize=(10, 4))
            names_cross = [r["Model"] for r in cross_rows]
            drops = [float(r["AUC Drop"]) for r in cross_rows]
            bar_colors = [GREEN if d > -0.05 else (AMBER if d > -0.15 else RED) for d in drops]
            bars = ax.bar(names_cross, drops, color=bar_colors, alpha=0.8, edgecolor=WHITE)
            ax.axhline(0, color=NAVY, lw=2)
            ax.axhline(-0.05, color=AMBER, lw=1.5, ls="--", alpha=0.8, label="Acceptable drop (-0.05)")
            ax.axhline(-0.15, color=RED, lw=1.5, ls="--", alpha=0.8, label="Poor generalization (-0.15)")
            for bar, d in zip(bars, drops):
                ax.text(bar.get_x()+bar.get_width()/2,
                        bar.get_height()-0.005 if d<0 else bar.get_height()+0.002,
                        f"{d:+.4f}", ha="center", va="top" if d<0 else "bottom",
                        fontsize=8, fontweight="bold")
            ax.set(title="AUC Drop: Voice Training â†’ Telemonitoring Test\n(Green = good generalization)",
                   ylabel="Î”AUC (test âˆ’ train)")
            ax.legend(fontsize=8); ax.grid(axis="y")
            ax.set_xticklabels(names_cross, rotation=20, ha="right")
            fig.tight_layout(); st.pyplot(fig); plt.close(fig)

            success_box(f"""âœ… Cross-dataset validation complete.
                Best generalizing model:
                <strong>{cross_rows[0]['Model']}</strong>
                with Telemonitoring AUC = <strong>{cross_rows[0]['Telemon AUC (test)']}</strong>
                and generalization status: <strong>{cross_rows[0]['Generalizes?']}</strong>""")
